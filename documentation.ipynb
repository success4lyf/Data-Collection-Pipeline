{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "\n",
    "class Scraper:\n",
    "    \"\"\"\n",
    "    This class provides the main functionality required to webscrape data\n",
    "    using Chrome Webdriver.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initaliser:\n",
    "        defines driver local location, &\n",
    "        defines website to be webscraped.\n",
    "        \"\"\"     \n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.url = 'https://www.boohoo.com/womens'\n",
    "        self.action = ActionChains(self.driver)\n",
    "        \n",
    "    def do_something(self):\n",
    "\n",
    "        self.driver.get(self.url)\n",
    "        self.accept_cookies()\n",
    "        self.create_directory()\n",
    "        self.navigate()\n",
    "        self.get_categories()\n",
    "        self.click_category()\n",
    "        self.get_details()\n",
    "        self.get_images()\n",
    "        self.driver.quit()\n",
    "\n",
    "    def accept_cookies(self):\n",
    "        try:\n",
    "            self.accept_cookies = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[@class=\"b-notification_panel-button b-button m-large\"]')))\n",
    "            self.accept_cookies.click()\n",
    "        except:\n",
    "            pass           \n",
    "        \n",
    "    def create_directory(self):\n",
    "        cwd = os.getcwd()\n",
    "        directory = ['Raw_Data', 'Images', 'Links']\n",
    "        for items in directory:                   \n",
    "            path = os.path.join(cwd, items)\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            print(\"Directory '% s' created\" % directory)\n",
    "\n",
    "    def navigate(self):\n",
    "        all_clothing = self.driver.find_element(By.XPATH, '(//li/a[@class=\"b-menu_bar-tab_content_link m-has-submenu \"])[3]')\n",
    "        self.action.move_to_element(all_clothing).perform()\n",
    "        time.sleep(2)\n",
    "\n",
    "    def get_categories(self):\n",
    "        self.categories = self.driver.find_elements(By.XPATH, '(//div[@aria-label=\"ALL CLOTHING\"])[2]/div/a')\n",
    "        self.category_names = []\n",
    "        self.category_links = []\n",
    "        with open('Links/Links.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for category in self.categories:\n",
    "                try:\n",
    "                    names = category.text\n",
    "                    links = category.get_attribute('href')                  \n",
    "                    self.category_names.append(names)\n",
    "                    self.category_links.append(links)\n",
    "                except NoSuchElementException:\n",
    "                    continue \n",
    "            writer.writerow(self.category_names)                       \n",
    "            writer.writerow(self.category_links)\n",
    "            print(self.category_names)\n",
    "            print(self.category_links)      \n",
    "\n",
    "    def click_category(self):\n",
    "        self.driver.get(self.category_links[1])\n",
    "        time.sleep(3)\n",
    "\n",
    "    def get_details(self):\n",
    "        self.items = self.driver.find_elements(By.XPATH, '//div[@class=\"l-plp_grid\"]/section')        \n",
    "        for item in self.items:\n",
    "            counter = 0\n",
    "            while counter < 40:\n",
    "                try:\n",
    "                    data = {\n",
    "                    'ID': str(uuid.uuid4()),\n",
    "                    'Title': item.find_element(By.XPATH, './/a[@class=\"b-product_tile-link\"]').text,\n",
    "                    'Price': item.find_element(By.XPATH, './/span[@class=\"b-price-item m-new\"]').text,\n",
    "                    'Discount': item.find_element(By.XPATH, './/span[@class=\"b-price-discount\"]').text,\n",
    "                    'Color': item.find_element(By.XPATH, './/img[@class=\"b-product_tile_swatches-swatch_image\"]').get_attribute('alt')              \n",
    "                    }\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "                print(data)\n",
    "                with open(f'Raw_Data/boohoo_{counter}.json', mode='w') as f:\n",
    "                    json.dump(data, f)\n",
    "                counter += 1\n",
    "            break\n",
    "            \n",
    "    def get_images(self):\n",
    "        \n",
    "        self.images = self.driver.find_elements(By.XPATH, '//div[@class=\"l-plp_grid\"]/section//picture/img')\n",
    "        counter = 0\n",
    "        for image in self.images[:40]:   \n",
    "            src = image.get_attribute('src')\n",
    "            website_opener = urllib.request.build_opener()\n",
    "            website_opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "            urllib.request.install_opener(website_opener)\n",
    "            try:\n",
    "                urllib.request.urlretrieve(src, f'Images/image_{counter}.png')                            \n",
    "            except NoSuchElementException:\n",
    "                continue              \n",
    "            counter += 1\n",
    "            print('image saved')            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    boohoo = Scraper()\n",
    "    boohoo.do_something()\n",
    "    # boohoo.navigate()\n",
    "    # boohoo.get_categories()\n",
    "    # boohoo.click_category()\n",
    "    # boohoo.get_details()   \n",
    "    # boohoo.get_images()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13303b6d4872171435f8bd6643c768c18bdd7a7b8df21633fcc425f742445c5e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('data_collection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
